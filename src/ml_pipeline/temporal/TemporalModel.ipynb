{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df03c07-69d7-421a-96b6-a996ad82eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a490fd-4d49-48d5-bb86-bc642ab33bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda29ed7-0cc1-4325-b725-58fc31ebb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/path/to/repo/explaining_cirrus\") # add path to where you cloned the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733d5eb-576c-4bb6-b5fa-00add0e6eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ExponentialLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from src.ml_pipeline.instantaneous.ml_preprocess import split_train_val_test, oh_encoding, CAT_VARS\n",
    "from src.ml_pipeline.temporal.lstm_model import LSTMRegressor, LogCallback\n",
    "from src.ml_pipeline.temporal.data_module import BacktrajDataModule, BacktrajDataset\n",
    "from src.ml_pipeline.temporal.custom_loss_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc213a40-f01f-4491-9d74-090567d2825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6e0bd-284d-463f-ba04-9d6303b2ee9e",
   "metadata": {},
   "source": [
    "# Train & Evaluate LSTM+Attention Model on Backtrajectory dataset\n",
    "\n",
    "We recommend using a machine with a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b6258-39f6-4c03-9ac2-9aaa51332a6a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c954d9-1bee-4c9e-8e4a-9c1be473f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPORAL_DATASET_PATH = \"/path/to/temporal_dataset\" # specify path where the temporal dataset is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c572c-7eed-4350-896a-3d6aa8772b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_months = [f\"{year}{month:02d}\" for year in range(2007,2010) for month in range(1,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600cda0-9975-4fd1-90e4-ec3a7c50010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"available months:\", all_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af6167-546a-4ca1-a2ed-e76e56851e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'lev': '<f8',\n",
    " 'lat': '<f8',\n",
    " 'lon': '<f8',\n",
    " 'latr': '<f8',\n",
    " 'lonr': '<f8',\n",
    " 'timestep': '<f8',\n",
    " 'trajectory_id': 'O',\n",
    " 'cloud_cover': '<f8',\n",
    " 't': '<f8',\n",
    " 'w': '<f8',\n",
    " 'u': '<f8',\n",
    " 'v': '<f8',\n",
    " 'rh_ice': '<f8',\n",
    " 'SO4': '<f4',\n",
    " 'land_water_mask': '<f8',\n",
    " 'season': 'O',\n",
    " 'nightday_flag': '<f8',\n",
    " 'instrument_flag': '<f8',\n",
    " 'dz_top_v2': '<f8',\n",
    " 'iwc': '<f8',\n",
    " 'icnc_5um': '<f8',\n",
    " 'reffcli': '<f8',\n",
    " 'lat_region': '<i8',\n",
    " 'lon_region': '<i8',\n",
    " 'DU': '<f4',\n",
    " 'DU_sub': '<f4',\n",
    " 'DU_sup': '<f4',\n",
    " 'wind_speed': '<f8',\n",
    " 'surface_height': '<f4',\n",
    " 'cloud_thickness_v2': '<f8',\n",
    " 'year_month': 'O'}\n",
    "parse_dates = ['time', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf011212-9b23-448c-b366-d28d4cd38b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# caution I: loading all months requires a lot of memory\n",
    "# caution II: a minimum of 8 months has to be selected when using train_size=0.8\n",
    "month_dfs = []\n",
    "for month in all_months:\n",
    "    print(month)\n",
    "    month_df = pd.read_csv(TEMPORAL_DATASET_PATH + f\"/temporal_{month}.csv\".format(month), dtype=dtypes, parse_dates=parse_dates)\n",
    "    month_dfs.append(month_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34a7a3-c7a0-4046-9103-7391b1adc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(month_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04736dbb-06ae-4d62-96d6-ad7c62cc15f5",
   "metadata": {},
   "source": [
    "## Define Hyperparameters and other config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b365d-d8c3-44f8-999a-51fd852c6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_features = [\"t\", \"w\", \"wind_speed\", \"DU_sup\", \"DU_sub\", 'SO4', \"surface_height\"]  # ,\"cc_traj\",\"IWC\",\"RWC\",\"LWC\",\"SWC\"]\n",
    "static_features = [\"land_water_mask\", \"season\", \"dz_top_v2\", \"cloud_thickness_v2\"]\n",
    "coord_vars = ['lev', 'lat', 'lon', 'latr', 'lonr', 'time', 'date', 'timestep', 'trajectory_id', 'cloud_cover']\n",
    "predictands = ['iwc', 'icnc_5um']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f69e2-bab7-44a9-a018-2c7903f58d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = dict(\n",
    "    predictands=predictands,  # list of predictands\n",
    "    sequential_features=sequential_features,  # list of temporally resolved features\n",
    "    static_features=static_features,  # list of static features\n",
    "    mtl_weighting_type=\"equal\", # weighting between predictand losses\n",
    "    criterion=RMSELoss(),  # initialized loss class\n",
    "    lstm_hparams={\"num_layers\": 1, \"hidden_size\": 250, \"dropout\": 0, \"attention\": True}, # lstm architecture choices\n",
    "    static_branch_hparams={\"layer_sizes\": [50], \"dropout\": 0.5, \"batchnorm\": True}, # static branch architecture choices\n",
    "    final_fc_layers_hparams={\"layer_sizes\": [100, 50], \"dropout\": 0.5, \"batchnorm\": True}, # final fc architecture choices\n",
    "    reweight=\"none\",  # if sample based reweighting (i.e deep imbalanced regression) â†’ reweighting mechansim\n",
    "    multiple_predictand_reweight_type=\"individual\", # if sample based reweighting combined with multi-task learning: calculating sample weights 'individual' for each predictand or based on 1 'lead_predictand'\n",
    "    reweight_lead_predictand=\"iwc\",\n",
    "    reweight_bin_width=10,\n",
    "    lds=False,\n",
    "    lds_kernel=\"gaussian\",\n",
    "    lds_ks=5,\n",
    "    lds_sigma=2,\n",
    "    data_filters=[],  # conditions dataframe will be filtered on e.g. cloud_cover>0.8\n",
    "    sequential_scaler=StandardScaler(),\n",
    "    static_scaler=StandardScaler(),\n",
    "    regional_feature_resolution=10,\n",
    "    backtraj_timesteps=48,\n",
    "    train_size=0.5,\n",
    "    batch_size=1000,\n",
    "    num_workers=1,\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler=False, # needs to be a learning_rate scheduler according to https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/core/lightning.html#LightningModule.configure_optimizers\n",
    "    grad_clip=0.5,\n",
    "    early_stopping=False,\n",
    "    max_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0926c-b1fa-4625-bf7b-147f1db15735",
   "metadata": {},
   "source": [
    "## Init, Train, Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f0588-5889-4fde-961f-980a85661304",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "if experiment_config[\"early_stopping\"]:\n",
    "    early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(monitor=\"val_loss\", min_delta=0.001,\n",
    "                                                                    patience=15)\n",
    "    callbacks.append(early_stop_callback)\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=experiment_config['max_epochs'],\n",
    "    log_every_n_steps=100,\n",
    "    progress_bar_refresh_rate=1000,\n",
    "    gradient_clip_val=experiment_config[\"grad_clip\"],\n",
    "    accelerator=\"gpu\",\n",
    "    devices=-1,\n",
    "    #     fast_dev_run=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6370e9-5331-4eba-ba45-37a5052041ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = BacktrajDataModule(\n",
    "    traj_df=df,\n",
    "    data_filters=experiment_config[\"data_filters\"],\n",
    "    predictands=experiment_config['predictands'],\n",
    "    sequential_features=experiment_config['sequential_features'],\n",
    "    static_features=experiment_config[\"static_features\"],\n",
    "    sequential_scaler=experiment_config['sequential_scaler'],\n",
    "    static_scaler=experiment_config['static_scaler'],\n",
    "    batch_size=experiment_config['batch_size'],\n",
    "    train_size=experiment_config[\"train_size\"],\n",
    "    num_workers=experiment_config[\"num_workers\"],\n",
    "    regional_feature_resolution=experiment_config[\"regional_feature_resolution\"],\n",
    "    reweight=experiment_config[\"reweight\"],\n",
    "    multiple_predictand_reweight_type=experiment_config[\"multiple_predictand_reweight_type\"],\n",
    "    reweight_lead_predictand=experiment_config[\"reweight_lead_predictand\"],\n",
    "    reweight_bin_width=experiment_config[\"reweight_bin_width\"],\n",
    "    lds=experiment_config[\"lds\"],\n",
    "    lds_kernel=experiment_config[\"lds_kernel\"],\n",
    "    lds_ks=experiment_config[\"lds_ks\"],\n",
    "    lds_sigma=experiment_config[\"lds_sigma\"],\n",
    "    backtraj_timesteps=experiment_config[\"backtraj_timesteps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed525b26-7641-4150-9604-535fd9f3050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMRegressor(\n",
    "    predictands=experiment_config[\"predictands\"],\n",
    "    n_sequential_features=len(dm.sequential_features),\n",
    "    n_static_features=len(dm.static_features),\n",
    "    lstm_hparams=experiment_config[\"lstm_hparams\"],\n",
    "    static_branch_hparams=experiment_config[\"static_branch_hparams\"],\n",
    "    final_fc_layers_hparams=experiment_config[\"final_fc_layers_hparams\"],\n",
    "    batch_size=experiment_config[\"batch_size\"],\n",
    "    learning_rate=experiment_config[\"learning_rate\"],\n",
    "    lr_scheduler=experiment_config[\"lr_scheduler\"],\n",
    "    criterion=experiment_config[\"criterion\"],\n",
    "    grad_clip=experiment_config[\"grad_clip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d74062-ff1f-47ed-883e-6f4a6725dde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### train model ###\n",
    "print(\"start training\")\n",
    "trainer.fit(model, dm)\n",
    "# evaluate\n",
    "trainer.test(model, datamodule=dm)\n",
    "torch.save(trainer.model.state_dict(), 'lstm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb839ab-d6fc-494b-9ce5-1e33c7996f02",
   "metadata": {},
   "source": [
    "## Plot Attention weights\n",
    "\n",
    "retrieve attention weights for test data and plot mean attention weight per time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60e3a1-1a74-4977-a9db-7bab924df3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting library\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "hv.extension('matplotlib')\n",
    "\n",
    "from bokeh.resources import INLINE\n",
    "import bokeh.io\n",
    "\n",
    "bokeh.io.output_notebook(INLINE)z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4c4e6-c768-4ea9-a895-0aed613a834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = dm.test_dataloader()\n",
    "train_dataloader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a8153-095f-4b05-9daa-f82c706de146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors of test dataset\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "X_seq_test=[]\n",
    "X_static_test=[]\n",
    "y_test=[]\n",
    "weights_test=[]\n",
    "coords_test=[]\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    X_seq, X_static, y, weights, coords = batch\n",
    "    X_seq_test.append(X_seq)\n",
    "    X_static_test.append(X_static)\n",
    "    y_test.append(y)\n",
    "    weights_test.append(weights)\n",
    "    coords_test.append(coords)\n",
    "    \n",
    "\n",
    "X_seq_test=torch.concat(X_seq_test)\n",
    "X_static_test=torch.concat(X_static_test)\n",
    "y_test = torch.concat(y_test)\n",
    "weights_test = torch.concat(weights_test)\n",
    "coords_test = torch.concat(coords_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b08da3-28a6-407e-b959-f12b8ad7f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lstm_out, (hn, cn) = model.lstm(X_seq_test) # lstm_out (N, T, hidden_size)\n",
    "    #lstm_out = lstm_out.to(torch.device,(\"cuda:0\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    z, alpha = model.attention_module(lstm_out)\n",
    "\n",
    "attention_weights = alpha.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1190f-3325-444f-b2a6-bb7aebf83513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc mean and sd of attention weight per timestep\n",
    "mean_attention = np.mean(attention_weights,axis=(0,2))\n",
    "sd_attention = np.std(attention_weights,axis=(0,2))\n",
    "\n",
    "attention_df = pd.DataFrame(columns=[\"mean\", \"lower\", \"upper\"])\n",
    "attention_df[\"mean\"] = mean_attention\n",
    "\n",
    "attention_df[\"lower\"] = mean_attention - 0.5 * sd_attention\n",
    "attention_df[\"upper\"] = mean_attention + sd_attention\n",
    "\n",
    "attention_df = attention_df.reset_index()\n",
    "attention_df = attention_df.rename(columns={\"index\":\"timestep\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb8895-2a34-46ea-bbee-d65f68ae1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create attention plot\n",
    "plt_options = {'fontsize': {'xlabel': '20px',\n",
    "  'ylabel': '15px',\n",
    "  'ticks': '15px',\n",
    "  'legend': '30px'},\n",
    " 'cmap': 'Colorblind',\n",
    " 'width': 900,\n",
    " 'height': 200,\n",
    " 'line_width':3       \n",
    "}\n",
    "\n",
    "attention_df.timestep -= 48\n",
    "attention_plt = attention_df.hvplot.line(x=\"timestep\", y=\"mean\", color=\"orange\", **plt_options) * attention_df.hvplot.area(x=\"timestep\",y=\"lower\",y2=\"upper\", color=\"orange\", line_alpha=0, fill_alpha=0.2, xlabel=\"timestep [h]\", ylabel=\"mean attention weight\", **plt_options)\n",
    "\n",
    "attention_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77feaec3-56ef-47cb-9212-ff7081cbc1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_xarray]",
   "language": "python",
   "name": "conda-env-pytorch_xarray-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
